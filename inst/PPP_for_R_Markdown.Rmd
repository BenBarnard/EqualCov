---
title: "A Comparison of Test Statistics for Equality of High-Dimensional Covariance Matrices using Dimension Reduction"
subtitle: "A Monte Carlo Power Simulation Study"
author: "Ben Barnard"
output: 
  ioslides_presentation:
    widescreen: TRUE
bibliography: presentation.bib
---

```{r setup, include = FALSE}
library(plyr)
library(stringr)
library(tidyverse)
library(EqualCov)
knitr::opts_chunk$set(echo = FALSE, message = FALSE, fig.width=10, fig.height=5.5)
options(warn = -1)
cbbPalette <- c("#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
```

```{r data, include = FALSE}
power <- read_csv("~/Box Sync/Dissert Data/power/powertotal.csv")
```

## Overview of Topics

- Introduction of the Problem

- Statistics for Testing Equality of High Dimensional Covariance Matrices

- Dimension Reduction via the Singular Value Decomposition

- Simulation Study Setup

- Results

- Summary 

- Future Work

- References

<div class="notes">
First, I will introduce the problem. Then, I will talk about some tests equality of covariance matrices for high dimensional data. I will then talk about some dimension reduction methods. The simulation Study Setup will follow with the results for the study being next. Last, I will summarize some findings and discuss future work.
</div>


# Introduction of the Problem

## Introduction

- Considering high-dimensional data

- Five test statistics for testing equality of covariance matrices

- Two dimension reduction methods via singular value decomposition

- Interested in comparing equality of 2 and $k$ covariance matrices

<div class="notes">
In this simulation I am considering high dimensional data. When I say high dimensional data I mean when the dimension/variables/features is larger than the number of samples. A common example would be genetic data from cancer patients where we could be looking at thousands of genes and tens of patients. I will present four equality of covariance matrix tests for high dimensional data and compare the tests with a modifed likelihood ratio test. Next, I will present two dimension reduction methods using singular value decomposition. For this presentation we are interested in the equality of 2 and k covariance matrices. More specifically we are testing 3 covariance matrices for k.
</div>

## Testing Equality of Covariances

- Let $\boldsymbol{\Sigma}_i \in \mathbb{R}_{p \times p}^>$ be the covariance matrix of the $i^\text{th}$ population for $i = 1, 2, \dots, k$. 

- We wish to test

$$H_0 : \boldsymbol{\Sigma}_1 = \ldots = \boldsymbol{\Sigma}_k.$$
- Our sample covariance matrices, $\boldsymbol{S}_1, \ldots \boldsymbol{S}_k$, are distributed $n_i \boldsymbol{S}_i \sim W_p(\boldsymbol{\Sigma}_i, n_i)$. 

      
<div class="notes">
For testing the equality of covariance matrices let sigma be the covariance matrix of the ith population for i from 1 to k of in our case 2 or 3. We wish to test the null hypothesis sigma one equals sigma two and so on and so on to sigma k. Our sample covariance matrices s one to s i are distributed n i S i wishart with mean sigma i and degrees of freedom n i but we aren't going to worry to much about this since we aren't finding any theoretical asymmptotic distributions for the tests, but this might be future work.
</div>



## Modified Likelihood Ratio Test

<br>

$$M = n \log |\boldsymbol{S}| - \sum \limits^k_{i=1} n_i \log |\boldsymbol{S}_i| \xrightarrow{d} \chi^2$$

<br>

This modified likelihood ratio test is only valid when $\boldsymbol{S}_i$ is nonsingular or when $n_i \gg m$.

<div class = "notes">
This is a modified likelihood ratio test. Some of you will recognize this as Boxe's M. This modified likelihood ratio test is only valid when the sample covariance matrix for the populations are nonsingular or when n i is much greater than m. This really won't work for high dimensional data.
</div>

# Statistics for Testing Equality of <br> High Dimensional Covariance Matrices

## Frobenius Norm

- Ledoit and Wolf 2004 showed some nice properties of the Frobenius norm:

$$
d^2 = \frac{tr \left( \boldsymbol{\Sigma}_i - \boldsymbol{\Sigma}_j \right)^2 }{p} = \frac{tr \left( \boldsymbol{\Sigma}_i^2 \right) }{p} + \frac{tr \left( \boldsymbol{\Sigma}_j^2 \right) }{p} - \frac{2tr \left( \boldsymbol{\Sigma}_i \boldsymbol{\Sigma}_j \right) }{p}.
$$

- Dividing by $p$ is not typical but allows the norm of the Identity to be 1.

- The norm is invariant to rotation.

<div class = "notes">
Ledoit and Wolf in 2004 showed some nice properties of the Frobenius norm which form most of the tests for equality of covariance matrices for high dimensional data have formulations that use the expansion terms. Some of you that know what the frobenius norm is are probably asking where did that p come from. This is not typical but dividing by p allows the norm of the identity to be one. This allows us also to see how different the average volume by dimension of the matrices. The Frobenius norm is also invariant to rotation which could be a problem if the matrices with the same volme are oriented in different ways.
</div>

## Schott 2007

- Schott proposed a test in 2007 based on the Frobenius norm.

<br>

$$ T_{Sc} = \sum \limits^{k}_{i < j} \frac{ \left( \hat{a}_{2i} + \hat{a}_{2j} - \frac{2}{p}tr \left( S_i S_j \right) \right) ^ 2}{\theta} \xrightarrow{d} \chi^2$$
<br>
$$\theta = 4 \hat{a}_2^2 \left( \sum \limits_{i < j}^ k \left( \frac{1}{n_i} + \frac{1}{n_j} \right) + (k - 1)(k - 2) \sum \limits_{i = 1}^k n_i^{-2} \right)$$

<div class = "notes">
Schott proposed a test in 2007 based off the Frobenius norm. This is not Schott's original formulation of the test that he proposed. Srivastava and Yanagihara in 2010 proposed a test which I am not showing but in that article they did some algebra and it up. In this test theta is just a consistent estimator for the variance of the numerator. Take note that there is an a hat two squared term in theta because I will bring this up again. 
</div>

## Schott 2007

- The $a_{ji}$ terms are consistent estimators of $\frac{tr\left( \boldsymbol{\Sigma}^j\right)}{p}$.

<br>

$$\hat{a}_{2i} = \frac{tr \left( \boldsymbol{V}_i^2 \right) - \frac{1}{n_i}tr \left( \boldsymbol{V}_i \right)^2}{ \left( n_i - 1 \right) \left(n_i + 2 \right)p} \xrightarrow{P} \frac{tr \left( \boldsymbol{\Sigma}_i^2 \right) }{p}$$
<br>

$$\hat{a}_2 = \frac{tr \left( \boldsymbol{V}^2 \right) - \frac{1}{n}tr \left( \boldsymbol{V} \right)^2 }{(n - 1)(n + 2)p} \xrightarrow{P} \frac{tr \left( \boldsymbol{\Sigma}^2 \right) }{p}$$

<div class = "notes">
The a terms are consistent estimators of the traces of the covariance parameter raised to a power divided by the dimension. So a hat 2 i is a consistent estimator of the trace of the covariance parameter squard divided by the dimension where V i is just the numerator of the sample covariance matrix and a hat 2 would be the pooled estimator where v would be the numerator of the pooled covariance.
</div>


## Chaipitak and Chongcharoen 2013

- Chaipitak and Chongcharoen proposed a test in 2013 based on a distance estimator $b = tr \left( \boldsymbol{\Sigma}^2_i \right) / tr \left( \boldsymbol{\Sigma}^2_j \right)$

$$T_{C} = \sum \limits^{k}_{i < j} \frac{ \left( \hat{b} - 1 \right)^2 }{\hat{\delta}^2} \xrightarrow{d} \chi^2$$
<br>

$$\hat{b} = \frac{\hat{a}_{2i}}{\hat{a}_{2j}} \quad \quad \hat{\delta}^2 = 4 \left( \frac{2\hat{a}_4}{p \hat{a}_2^2} \sum \limits^k_{i=1} \frac{1}{n_i -1} + \sum \limits^k_{i=1} \frac{1}{ \left( n_i - 1 \right)^2 } \right)$$

<div class = "notes">
Chaipitak and Chongcharoen proposed a test in 2013 based on a distance estimator b which is just the ratio of of the first expansion terms of the Frobenius norm. and delta hat squared is a consistent estimator of the variance of the numerator. a hat four is a consistent estimator for the trace of the pooled covariance parameter to the fourth power divided by the dimension.
</div>


## Srivastava et al. 2014

- Srivastava et al. proposed a test in 2014 that had the same form as Schott 2007 but used an unbiased consistent estimator for $\hat{a}_{2i}$.

<br>

$$ T_{S14} = \sum \limits^{k}_{i < j} \frac{ \left( \hat{a}_{2i} + \hat{a}_{2j} - \frac{2}{p}tr \left( \boldsymbol{S}_i \boldsymbol{S}_j \right) \right) ^ 2}{\theta} \xrightarrow{d} \chi^2$$
<br>



<div class = "notes">
Srivastava et al. proposed a test in 2014 that had the same form as Schott 2007 but used an unbiased consistent estimator for a hat 2 i.
</div>


## Srivastava et al. 2014

<br>

$$\hat{a}_{2i} = \frac{ \left(n_i -2 \right) \left( n_i -1 \right) tr \left( \boldsymbol{V}_i^2 \right) - n \left( n - k \right) tr \left( \boldsymbol{D}^2_i \right) + tr \left( \boldsymbol{V}_i \right)^2 }{pn_i \left( n_i -1 \right) \left( n_i -2 \right) \left( n_i -3 \right) } \xrightarrow{P} \frac{tr \left( \boldsymbol{\Sigma}^2 \right) }{p}$$

<br>

$$\boldsymbol{A}_i = \boldsymbol{X}_i^T\boldsymbol{X}_i \quad \quad \boldsymbol{D}_i = diag \left(  a_{ij}\right)$$
<div class = "notes">
This is our new unbiased consistent estimator of a hat 2 i where v i is still the numerator of the sample covariance matrix. For D if the numerator for the sample covariance for our data is the matrix x i x i transpose then A i is xi transpose x i. D i is the diagonalization of the elements of the matrix A i, or just the sum of its elements.
</div>

## Ishii et al. 2016

- Ishii et al proposed their test in 2016 using ratios of the first eigenvalues and first eigenvectors.

<br>

$$T_I = \prod \limits^{k}_{i < j} \tilde{\lambda}_* \tilde{h}_* \tilde{\gamma}_* \xrightarrow{d} F$$

<br>

- Components of the test use noise reduced values since the largest eigenvalue is biased upwards with high dimensional data.

<div class = "notes">
Ishii et al proposed their test in 2016 using ratios of the first eigenvalues and first eigenvectors. Components of the test use noise reduced values since the largest eigenvalue is biased upwards with high dimensional data. It gets worse.
</div>

## Ishii et al. 2016

- The first noise reduced eigenvalue is solved by using the sum of the other non zero eigenvalues of the covariance matrix.

<br>

$$\tilde{\lambda}_* = \frac{max \left( \tilde{\lambda}_{i1}, \tilde{\lambda}_{j1} \right) }{min \left( \tilde{\lambda}_{i1}, \tilde{\lambda}_{j1} \right) }$$

<br>

$$\tilde{\lambda}_{i1} = \hat{\lambda}_{i1} - \frac{tr \left( S_i \right) - \hat{\lambda}_{i1}}{n_i - 2}$$

<div class = "notes">
Lambda tilde star is the max of the first noise reduced eigenvalues of group i and group j divided by the minimum. Where the first noised reduced eigenvalue is just the first eigenvalue of the sample covariance minus the sum of all the other eigenvalues divided by the sample size of the group minus two.
</div>

## Ishii et al. 2016

- The noise reduced first eigenvector is solved using the first noise reduced eigenvalue.

<br>

$$\tilde{h}_* = max \left( \left|\tilde{h}^T_i \tilde{h}_j \right| , \left| \tilde{h}^T_i \tilde{h}_j \right|^{-1} \right) $$

$$\tilde{h}_{i1} = \left\{ (n -1) \tilde{\lambda}_{i1}\right\}^{-1/2}\left( \boldsymbol{X}_i - \overline{\boldsymbol{X}}_i\right) \hat{\boldsymbol{\mu}}_{i1}$$

<div class = "notes">
h tilde the noise reduced first eigenvector is solved using the noised reduced first eigenvalue. h tilde star is the max of the scalar from the first noise reduced eigenvectors times each other or the inverse. We find the first noise reduced eigenvector by using the first noise reduced eigenvalue by using this equation.
</div>

## Ishii et al. 2016

- The final part of the test consists of the sum of all the other non zero eigenvalues of the covariance matrix.

<br>

$$\tilde{\gamma}_* = max \left( \frac{\tilde{\kappa}_i}{\tilde{\kappa}_j}, \frac{\tilde{\kappa}_j}{\tilde{\kappa}_i} \right) $$

<br>

$$\tilde{\kappa}_i = tr \left( S_{i} \right) - \tilde{\lambda}_{i1}$$

<div class = "notes">
The final part of the test consists of the sum of all the other non zero eigenvalues of the covariance matrix. so kappa is the sum of the eigenvalues of the sample covariance minus the first noise reduced eigenvalue. Gamma tilde star is the max of ratio of the kappa tildes.
</div>
  
# Dimension Reduction via the Singular Value Decomposition

## Data Scatter Matrix 

- For samples $\boldsymbol{X}_i$, let 
  $\boldsymbol{X} = \left[ \boldsymbol{X}_1 \vdots \ \ldots \ \vdots \boldsymbol{X}_i\vdots \ \ldots \ \vdots \boldsymbol{X}_k \right]^T$.

- Let $\boldsymbol{M}$ represent the scatter matrix of $\boldsymbol{X}$

- Let $\boldsymbol{M} = \boldsymbol{UDU}^T$ represent the singular value decompostion of $\boldsymbol{M}$.

- Partition $\boldsymbol{U}$ such that $\boldsymbol{U} = \left[ \boldsymbol{U}_1 \vdots \, \boldsymbol{U}_2 
  \right]$ with $\boldsymbol{U}_1 \in \mathbb{R}^{p \times q}$, where $q = 1, 2, \dots , p$.

- Project $\boldsymbol{X}_i$ to $q$ dimensions by taking $\boldsymbol{X}_{Ri}^T = \boldsymbol{U}_1^T 
  \boldsymbol{X}_i^T$.
  
  <div class = "notes">
  I tried four different dimension reduction methods using the singular value decomposition but I am only going to present two of those methods. Mostly for time sake and to clean up the graphs. First, there is the Data Scatter matrix.  For populations X i let X be the concatination of the X is. Then let M be the scatter matrix of X thus UDU transpose represents the singular value decomposition of M. We then partition U by taking the sigular vectors associated with the largest q singular values. We then project each X i into q dimensions with our dimension reduction matrix.
  </div>
  
## Covariance Difference Concatination Matrix 

- For groups $\boldsymbol{X}_i$ let $\boldsymbol{S_i}$ be the sample covariance matrix.

- Let $\widehat{\boldsymbol{M}} := \left[ \boldsymbol{S}_2 - \boldsymbol{S}_1 \vdots \ldots \vdots \boldsymbol{S}_i - \boldsymbol{S}_1 \vdots \ldots \vdots \boldsymbol{S}_k - \boldsymbol{S}_1 \right]$ where $i = 1, \ldots, k$ groups.

- Let $\widehat{\boldsymbol{M}} = \boldsymbol{UDV}^T$ represent the singular value decompostion of $\widehat{\boldsymbol{M}}$.

- Partition $\boldsymbol{U}$ such that $ \boldsymbol{U} = \left[ \boldsymbol{U}_1 \vdots, \boldsymbol{U}_2 
  \right] $  with $\boldsymbol{U}_1 \in \mathbb{R}^{p \times q}$.

- Project $\boldsymbol{X}_i$ to $q$ dimensions by taking $\boldsymbol{X}_{Ri}^T = \boldsymbol{U}_1^T 
  \boldsymbol{X}_i^T$.
  
  <div class = "notes">
  The next method is going to be the covariance difference concatination matrix. For groups X i let s i be the sample covariance matrix. Then, let M be the concatination of the the covariance differences. Next, let udv be the singular value decomposition of m. We then partition U by taking the sigular vectors associated with the largest q singular values. We then project each X i into q dimensions with our dimension reduction matrix.
  </div>

# Simulation Study Setup

## Critical Value Simulation

The critical value data sets generated for the simulation study have the following characteristics:

- $n_i = 15$

- $p = 100$

- We generated the $k$ populations from $\mathcal{N}_p \left( \boldsymbol{0}, \boldsymbol{\Sigma}_i \right)$.

- The number of repetitions is 100,000.

- $cv = inf\{x  \in  \mathbb{R}  :  1 - \alpha  \leq  \hat{F}_{T}(x) \}$, where $\alpha = .05$.

<div class = "notes">
The critical value data sets generated for the simulation study have the following characteristics n i equals 15 dimensions equals p. Next, we generated the k populations from multivariate normal with mean vector 0 and covariance matrix sigma i. We generated 100,000 repetions. Thus with an alpha of .05 our critical value will be this with f hat of x being the empirical cdf of the tests from the simulated data sets. We used a spiked covariance structure or in other words if we were to find the eigenvalues of the covariance matrix the largest eigenvalue would be exremely large and the trend would quickly appear to be decreasing to zero. So this trend should be even more exaggerated with high dimensional data. We also used a toeplitz structure covariance matrix.
</div>

## Power Simulation

The power simulation data sets generated for the simulation study have the following characteristics:

- $n_i = 15$

- $p = 100$

- We generated the $k$ populations from $\mathcal{N}_p \left( \boldsymbol{0}, \boldsymbol{\Sigma}_i \right)$, and $\mathcal{N}_p \left( \boldsymbol{0}, \boldsymbol{\Sigma}_i * c \right)$.

- Number of repetitions is 1,000.

$$Power = \frac{\sum \limits^{1000}_{i = 1} I(T \geq cv)}{1000}$$
<div class = "notes">
The power simulation data sets generated for the simulation study have the following characteristics: n i 15 and dimensions 100. We generated the k populations from a multivariate normal with mean vectors 0 and covariances sigma i and sigma i times some constant c. I only changed one matrix so with three populations only one population has the covariance of sigma i time c and the other two are just sigma i. For right now I chose to just multiply by some constant c as to not change the structure of the eigenvalues of the covariance matrices and the frobenius norm might not detect diferences caused by other changes. I also didn't want to keep you for five days. This will just scale the eigenvalues upward. I will not go to in depth what values c takes on since we are going to look at how power increases with the frobenius norm so just know as c increases so should the frobenius norm. 
</div>

## Toeplitz Covariance Structure

<br>

$$
  \boldsymbol{\Sigma} = \begin{bmatrix}
  1 & .5 & .25 & .125 \\
  .5 & 1 & .5 & .25 \\
  .25 & .5 & 1 & .5 \\
  .125 & .25 & .5 & 1 
  \end{bmatrix}.
  $$
<div class = "notes">
This is the general form of the toeplitz matrix I used but with 100 dimensions.
</div>


## Toeplitz Covariance Structure

```{r}
eig <- eigen(toeplitz(.5 ^ seq(0, (100 - 1))))$values
df <- data_frame(Eigenvalues = eig,
                 Index = seq(1, length(eig), 1))
ggplot(df) +
  geom_point(aes(x = Index, y = Eigenvalues)) +
  theme_bw() +
  scale_colour_manual(values = cbbPalette, guide = guide_legend(title = "Test")) + 
  theme(axis.text.x = element_text(size = 12),
        axis.title.x = element_text(size = 16),
        axis.text.y = element_text(size = 12),
        axis.title.y = element_text(size = 16),
        legend.text = element_text(size = 12),
        legend.title = element_text(size = 16),
        strip.text = element_text(size = 12))
```

<div class = "notes">
The plot of the eigenvalues looks like this.
</div>
  
  
## Spiked Covariance Structure

<br>
  
  $$
  \boldsymbol{\Sigma} = \begin{bmatrix}
  20      & 1      & 3      & 3      & 3      & \ldots  & 3\\
  1       & \ddots & \ddots & 3      & \vdots & \ddots  & \vdots\\
  3       & \ddots & 20     & 1      & 3      & \ldots  & 3\\
  3       & 3      & 1      & 5      & 4      & \ldots  & 4 \\
  3       & \ldots & 3      & 4      & \ddots & \ddots  & \vdots \\
  \vdots  & \ddots & \vdots & \vdots & \ddots & \ddots  & 4\\
  3       & \ldots & 3      & 4      & \ldots & 4       & 5
  \end{bmatrix}.
  $$
  
<div class = "notes">
This is the structure of the spiked covariance matrix I used.  
</div>
  
## Spiked Covariance Structure

```{r}
eig <- eigen(cov_maker(keepers = list(c(20, 1, rep(3, 8))),
                             offs = list(3, nrow = 10, ncol = 100 - 10),
                             losers = list(c(5, rep(4, 100 - 11)))))$values
df <- data_frame(Eigenvalues = eig,
                 Index = seq(1, length(eig), 1))
ggplot(df) +
  geom_point(aes(x = Index, y = Eigenvalues)) +
  theme_bw() +
  scale_colour_manual(values = cbbPalette, guide = guide_legend(title = "Test")) + 
  theme(axis.text.x = element_text(size = 12),
        axis.title.x = element_text(size = 16),
        axis.text.y = element_text(size = 12),
        axis.title.y = element_text(size = 16),
        legend.text = element_text(size = 12),
        legend.title = element_text(size = 16),
        strip.text = element_text(size = 12))
```

<div class = "notes">
The plot of the eigenvalues looks like this.
</div>


# Results

# Two Populations

## Toeplitz Covariance No Dimension Reduction

```{r}
powertoe <- rename(filter(power, type == "toeplitz"), `Reduction Method` = Reduction.Method, Power = power)
ggplot(data = filter(powertoe,
                     populations == 2,
                     originaldimensions == 100,
                     ReducedDimension == 100,
                     Samples == 15,
                     Test %in% c("Modified Likelihood Ratio", "Chaipitak and Chongcharoen 2013", 
"Ishii et al. 2016", "Schott 2007", "Srivastava et al. 2014"),
                     `Reduction Method` %in% c("None"),
                     ReducedDimension %in% c(5, 100))) +
  geom_line(aes(x = `Frobenius Norm`, y = Power, color = Test), size = 1.5) +
  theme_bw() +
  scale_colour_manual(values = cbbPalette, guide = guide_legend(title = "Test")) + 
  theme(axis.text.x = element_text(size = 12),
        axis.title.x = element_text(size = 16),
        axis.text.y = element_text(size = 12),
        axis.title.y = element_text(size = 16),
        legend.text = element_text(size = 12),
        legend.title = element_text(size = 16),
        strip.text = element_text(size = 12)) +
  ylim(c(0, 1))
```

## Toeplitz Covariance Dimension Reduction

```{r}
ggplot(data = filter(powertoe,
                     populations == 2,
                     originaldimensions == 100,
                     Samples == 15,
                     Test %in% c("Modified Likelihood Ratio", "Chaipitak and Chongcharoen 2013", 
"Ishii et al. 2016", "Schott 2007", "Srivastava et al. 2014"),
                     `Reduction Method` %in% c("Covariance Differences", "Data Scatter"),
                     ReducedDimension %in% c(5, 100))) +
  geom_line(aes(x = `Frobenius Norm`, y = Power, color = Test), size = 1.5) +
  facet_grid(`Reduction Method` ~ ReducedDimension) +
  theme_bw() +
  scale_colour_manual(values = cbbPalette, guide = guide_legend(title = "Test")) + 
  theme(axis.text.x = element_text(size = 12),
        axis.title.x = element_text(size = 16),
        axis.text.y = element_text(size = 12),
        axis.title.y = element_text(size = 16),
        legend.text = element_text(size = 12),
        legend.title = element_text(size = 16),
        strip.text = element_text(size = 12))+
  ylim(c(0, 1))
```

## Toeplitz Covariance at a Frobenius Norm of 0.01

```{r}
ggplot(data = filter(powertoe,
                     populations == 2,
                     originaldimensions == 100,
                     Samples == 15,
                     Test %in% c("Modified Likelihood Ratio", "Chaipitak and Chongcharoen 2013", 
"Ishii et al. 2016", "Schott 2007", "Srivastava et al. 2014"),
                     `Reduction Method` %in% c("Covariance Differences", "Data Scatter"),
                     `Frobenius Norm` >= 0.01,
                     `Frobenius Norm` <= 0.014,
                     ReducedDimension <= 25)) +
  geom_line(aes(x = ReducedDimension, y = Power, color = Test), size = 1.5) +
  facet_wrap(~ `Reduction Method`) +
  theme_bw()  +
  scale_colour_manual(values = cbbPalette, guide = guide_legend(title = "Test")) + 
  theme(axis.text.x = element_text(size = 12),
        axis.title.x = element_text(size = 16),
        axis.text.y = element_text(size = 12),
        axis.title.y = element_text(size = 16),
        legend.text = element_text(size = 12),
        legend.title = element_text(size = 16),
        strip.text = element_text(size = 12))+
  ylim(c(0, 1))
```


## Spiked Covariance No Dimension Reduction

```{r}
powerell <- rename(filter(power, type == "elliptical"), `Reduction Method` = Reduction.Method, Power = power)
ggplot(data = filter(powerell,
                     populations == 2,
                     originaldimensions == 100,
                     ReducedDimension == 100,
                     Samples == 15,
                     Test %in% c("Modified Likelihood Ratio", "Chaipitak and Chongcharoen 2013", 
"Ishii et al. 2016", "Schott 2007", "Srivastava et al. 2014"),
                     `Reduction Method` %in% c("None"),
                     ReducedDimension %in% c(5, 100))) +
  geom_line(aes(x = `Frobenius Norm`, y = Power, color = Test), size = 1.5) +
  theme_bw()  +
  scale_colour_manual(values = cbbPalette, guide = guide_legend(title = "Test")) + 
  theme(axis.text.x = element_text(size = 12),
        axis.title.x = element_text(size = 16),
        axis.text.y = element_text(size = 12),
        axis.title.y = element_text(size = 16),
        legend.text = element_text(size = 12),
        legend.title = element_text(size = 16),
        strip.text = element_text(size = 12))+
  ylim(c(0, 1))
```

## Spiked Covariance Dimension Reduction

```{r}
ggplot(data = filter(powerell,
                     populations == 2,
                     originaldimensions == 100,
                     Samples == 15,
                     Test %in% c("Modified Likelihood Ratio", "Chaipitak and Chongcharoen 2013", 
"Ishii et al. 2016", "Schott 2007", "Srivastava et al. 2014"),
                     `Reduction Method` %in% c("Covariance Differences", "Data Scatter"),
                     ReducedDimension %in% c(5, 100))) +
  geom_line(aes(x = `Frobenius Norm`, y = Power, color = Test), size = 1.5) +
  facet_grid(`Reduction Method` ~ ReducedDimension) +
  theme_bw()  +
  scale_colour_manual(values = cbbPalette, guide = guide_legend(title = "Test")) + 
  theme(axis.text.x = element_text(size = 12),
        axis.title.x = element_text(size = 16),
        axis.text.y = element_text(size = 12),
        axis.title.y = element_text(size = 16),
        legend.text = element_text(size = 12),
        legend.title = element_text(size = 16),
        strip.text = element_text(size = 12))+
  ylim(c(0, 1))
```

## Spiked Covariance at a Frobenius Norm of 0.7

```{r}
ggplot(data = filter(powerell,
                     populations == 2,
                     originaldimensions == 100,
                     Samples == 15,
                     Test %in% c("Modified Likelihood Ratio", "Chaipitak and Chongcharoen 2013", 
"Ishii et al. 2016", "Schott 2007", "Srivastava et al. 2014"),
                     `Reduction Method` %in% c("Covariance Differences", "Data Scatter"),
                     `Frobenius Norm` >= 0.75,
                     `Frobenius Norm` <= 0.85,
                     ReducedDimension <= 25)) +
  geom_line(aes(x = ReducedDimension, y = Power, color = Test), size = 1.5) +
  facet_wrap(~ `Reduction Method`) +
  theme_bw()  +
  scale_colour_manual(values = cbbPalette, guide = guide_legend(title = "Test")) + 
  theme(axis.text.x = element_text(size = 12),
        axis.title.x = element_text(size = 16),
        axis.text.y = element_text(size = 12),
        axis.title.y = element_text(size = 16),
        legend.text = element_text(size = 12),
        legend.title = element_text(size = 16),
        strip.text = element_text(size = 12))+
  ylim(c(0, 1))
```


# Three Populations

## Toeplitz Covariance No Dimension Reduction

```{r}
ggplot(data = filter(powertoe,
                     populations == 3,
                     originaldimensions == 100,
                     ReducedDimension == 100,
                     Samples == 15,
                     Test %in% c("Modified Likelihood Ratio", "Chaipitak and Chongcharoen 2013", 
"Ishii et al. 2016", "Schott 2007", "Srivastava et al. 2014"),
                     `Reduction Method` %in% c("None"),
                     ReducedDimension %in% c(5, 100))) +
  geom_line(aes(x = `Frobenius Norm`, y = Power, color = Test), size = 1.5) +
  theme_bw()  +
  scale_colour_manual(values = cbbPalette, guide = guide_legend(title = "Test")) + 
  theme(axis.text.x = element_text(size = 12),
        axis.title.x = element_text(size = 16),
        axis.text.y = element_text(size = 12),
        axis.title.y = element_text(size = 16),
        legend.text = element_text(size = 12),
        legend.title = element_text(size = 16),
        strip.text = element_text(size = 12))+
  ylim(c(0, 1))
```

## Toeplitz Covariance Dimension Reduction

```{r}
ggplot(data = filter(powertoe,
                     populations == 3,
                     originaldimensions == 100,
                     Samples == 15,
                     Test %in% c("Modified Likelihood Ratio", "Chaipitak and Chongcharoen 2013", 
"Ishii et al. 2016", "Schott 2007", "Srivastava et al. 2014"),
                     `Reduction Method` %in% c("Covariance Differences", "Data Scatter"),
                     ReducedDimension %in% c(5, 100))) +
  geom_line(aes(x = `Frobenius Norm`, y = Power, color = Test), size = 1.5) +
  facet_grid(`Reduction Method` ~ ReducedDimension) +
  theme_bw()  +
  scale_colour_manual(values = cbbPalette, guide = guide_legend(title = "Test")) + 
  theme(axis.text.x = element_text(size = 12),
        axis.title.x = element_text(size = 16),
        axis.text.y = element_text(size = 12),
        axis.title.y = element_text(size = 16),
        legend.text = element_text(size = 12),
        legend.title = element_text(size = 16),
        strip.text = element_text(size = 12))+
  ylim(c(0, 1))
```

## Toeplitz Covariance at a Frobenius Norm of 0.01

```{r}
ggplot(data = filter(powertoe,
                     populations == 3,
                     originaldimensions == 100,
                     Samples == 15,
                     Test %in% c("Modified Likelihood Ratio", "Chaipitak and Chongcharoen 2013", 
"Ishii et al. 2016", "Schott 2007", "Srivastava et al. 2014"),
                     `Reduction Method` %in% c("Covariance Differences", "Data Scatter"),
                     `Frobenius Norm` >= 0.015,
                     `Frobenius Norm` <= 0.02,
                     ReducedDimension <= 25)) +
  geom_line(aes(x = ReducedDimension, y = Power, color = Test), size = 1.5) +
  facet_wrap(~ `Reduction Method`) +
  theme_bw()  +
  scale_colour_manual(values = cbbPalette, guide = guide_legend(title = "Test")) + 
  theme(axis.text.x = element_text(size = 12),
        axis.title.x = element_text(size = 16),
        axis.text.y = element_text(size = 12),
        axis.title.y = element_text(size = 16),
        legend.text = element_text(size = 12),
        legend.title = element_text(size = 16),
        strip.text = element_text(size = 12))+
  ylim(c(0, 1))
```



## Spiked Covariance No Dimension Reduction

```{r}
ggplot(data = filter(powerell,
                     populations == 3,
                     originaldimensions == 100,
                     ReducedDimension == 100,
                     Samples == 15,
                     Test %in% c("Modified Likelihood Ratio", "Chaipitak and Chongcharoen 2013", 
"Ishii et al. 2016", "Schott 2007", "Srivastava et al. 2014"),
                     `Reduction Method` %in% c("None"),
                     ReducedDimension %in% c(5, 100))) +
  geom_line(aes(x = `Frobenius Norm`, y = Power, color = Test), size = 1.5) +
  theme_bw()  +
  scale_colour_manual(values = cbbPalette, guide = guide_legend(title = "Test")) + 
  theme(axis.text.x = element_text(size = 12),
        axis.title.x = element_text(size = 16),
        axis.text.y = element_text(size = 12),
        axis.title.y = element_text(size = 16),
        legend.text = element_text(size = 12),
        legend.title = element_text(size = 16),
        strip.text = element_text(size = 12))+
  ylim(c(0, 1))
```

## Spiked Covariance Dimension Reduction

```{r}
ggplot(data = filter(powerell,
                     populations == 3,
                     originaldimensions == 100,
                     Samples == 15,
                     Test %in% c("Modified Likelihood Ratio", "Chaipitak and Chongcharoen 2013", 
"Ishii et al. 2016", "Schott 2007", "Srivastava et al. 2014"),
                     `Reduction Method` %in% c("Covariance Differences", "Data Scatter"),
                     ReducedDimension %in% c(5, 100))) +
  geom_line(aes(x = `Frobenius Norm`, y = Power, color = Test), size = 1.5) +
  facet_grid(`Reduction Method` ~ ReducedDimension) +
  theme_bw()  +
  scale_colour_manual(values = cbbPalette, guide = guide_legend(title = "Test")) + 
  theme(axis.text.x = element_text(size = 12),
        axis.title.x = element_text(size = 16),
        axis.text.y = element_text(size = 12),
        axis.title.y = element_text(size = 16),
        legend.text = element_text(size = 12),
        legend.title = element_text(size = 16),
        strip.text = element_text(size = 12)) +
  ylim(c(0, 1))
```

## Spiked Covariance at a Frobenius Norm of 0.9

```{r}
ggplot(data = filter(powerell,
                     populations == 3,
                     originaldimensions == 100,
                     Samples == 15,
                     Test %in% c("Modified Likelihood Ratio", "Chaipitak and Chongcharoen 2013", 
"Ishii et al. 2016", "Schott 2007", "Srivastava et al. 2014"),
                     `Reduction Method` %in% c("Covariance Differences", "Data Scatter"),
                     `Frobenius Norm` >= 0.95,
                     `Frobenius Norm` <= 1.05,
                     ReducedDimension <= 25)) +
  geom_line(aes(x = ReducedDimension, y = Power, color = Test), size = 1.5) +
  facet_wrap(~ `Reduction Method`) +
  theme_bw()  +
  scale_colour_manual(values = cbbPalette, guide = guide_legend(title = "Test")) + 
  theme(axis.text.x = element_text(size = 12),
        axis.title.x = element_text(size = 16),
        axis.text.y = element_text(size = 12),
        axis.title.y = element_text(size = 16),
        legend.text = element_text(size = 12),
        legend.title = element_text(size = 16),
        strip.text = element_text(size = 12)) + 
    ylim(c(0,1))
```

# Summary and References

## Summary

- We saw improvements in power with dimension reduction.

- It appears the high dimension tests don't do that well with spiked covariance matrices and dimension reduction method doesn't seem to help that much.

- Dimension reduction with the modified likelihood ratio test improved the power and appeared to beat the high dimensional tests when dealing with a spiked covariance matrix.

<div class = "notes">
We saw improvements in power with dimension reduction. It appears the high dimension tests don't do that well with spiked covariance matrices and dimension reduction method doesn't seem to help that much. Dimension reduction with the modified likelihood ratio test improved the power and appeared to beat the high dimensional tests when dealing with a spiked covariance matrix.
</div>

## Future Work

- Explore some of the asymptotic properties of the tests little better.

- Why does the Data Scatter dimension reduction method appear to do better than the covariance differences method.

- Looking at a another nontrivial k $(k \neq 3)$ group situation.

- Look at the Wald test with dimension reduction.

- Seeing how shrinkage estimators perform with these tests.

- Looking at one sample high dimensional tests.

<div class = "notes">
Explore some of the asymptotic properties of the tests little better. Why does the Data Scatter dimension reduction method appear to do better than the covariance differences method. Looking at a another nontrivial k $(k \neq 3)$ group situation. Look at the Wald test with dimension reduction. Seeing how shrinkage estimators perform with these tests. Looking at one sample high dimensional tests.
</div>

## References {.smaller}

  @schott_test_2007, @srivastava_tests_2014, @ishii_asymptotic_2016, 
  @chaipitak_test_2013, @ledoit_well-conditioned_2004
  
