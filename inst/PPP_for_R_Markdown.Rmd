---
title: "A Comparison of Tests Statistics for Equality of High-Dimensional Covariance Matrices"
subtitle: "A Monte Carlo Power Simulation Study"
author: "Ben Barnard"
output: 
  ioslides_presentation:
    widescreen: TRUE
bibliography: presentation.bib
---

```{r setup, include = FALSE}
library(plyr)
library(stringr)
library(tidyverse)
knitr::opts_chunk$set(echo = FALSE)
options(warn = -1)
```

```{r data, include = FALSE}
power <- read_csv("~/Box Sync/Dissert Data/power/powertotal.csv")
```

## Overview of Topics

- Introduction of the problem

- Statistics for Testing Equality of High Dimensional Covariance Matrices

- Dimension Reduction via the Singular Value Decomposition

- Simulation study setup

- Results

- Summary and References


# Introduction of the problem

## Introduction

- Considering high-dimensional data

- Interested in comparing equality of 2 and k (3) covariance matrices

- Five test statistics for testing equality of covariance matrices

- Two dimension reduction methods via singular value decomposition

## Testing Equality of Covariances

If the $m \times m$ covariance matrices from the ith population is $\Sigma_i$, we wish to test

$$H_0 : \Sigma_1 = \ldots = \Sigma_g.$$
Then our independent estimates of $\Sigma_1, \ldots , \Sigma_g$, $S_1, \ldots S_g$ are $n_i S_i \sim W_m(\Sigma_i, n_i)$. 

We aren't going to worry to much about this since we aren't finding any asymmptotic distributions for the tests, but this might be future work. 

## Modified Likelihood Ratio Test

<br>

$$M = n \log |S| - \sum \limits^g_{i=1} n_i \log |S_i|$$

<br>

This modified likelihood ratio test is only valid when $S_i$ is nonsingular or when $n_i \gg m$.

## Wald Test

<br>

$$W = \frac{n}{2} \left \{ \sum \limits^g_{i = 1}\frac{n_i}{n} tr(S_iS^{-1}S_iS^{-1}) - \sum \limits^g_{i=1}\sum \limits^g_{j=1}\frac{n_in_j}{n^2}tr(S_iS^{-1}S_jS^{-1})  \right \}$$

<br>

This wald test is only valid when $S$ is nonsingular or when $n \gg m$.

# Statistics for Testing Equality of High Dimensional Covariance Matrices

## Frobenius Norm

- Ledoit and Wolf 2004 showed some nice properties of the frobenius norm:

$$
d^2 = \frac{tr(\Sigma_i - \Sigma_j)^2}{p} = \frac{tr(\Sigma_i^2)}{p} + \frac{tr(\Sigma_j^2)}{p} - \frac{2tr(\Sigma_i \Sigma_j)}{p}.
$$

- Dividing by $p$ is not typical but allows the norm of the Identity to be 1.

- The norm is invariant by rotation. Problem?

## Schott 2007

- Schott proposed a test in 2007 based on the frobenius norm.

<br>

$$ T_{Sc} = \sum \limits^{k}_{i < j} \frac{(\hat{a}_{2i} + \hat{a}_{2j} - \frac{2}{p}tr(S_i S_j)) ^ 2}{\theta}$$
<br>
$$\theta = 4 \hat{a}_2^2 \left( \sum \limits_{i < j}^ k (\frac{1}{n_i} + \frac{1}{n_j}) + (k - 1)(k - 2) \sum \limits_{i = 1}^k n_i^{-2} \right)$$

## Schott 2007

- The $a$ terms are consistent estimators.

<br>

$$\hat{a}_{2i} = \frac{tr(V_i^2) - \frac{1}{n_i}tr(V_i)^2}{(n_i - 1)(n_i + 2)p} \xrightarrow{P} \frac{tr(\Sigma_i^2)}{p}$$
<br>

$$\hat{a}_2 = \frac{tr(V^2) - \frac{1}{n}tr(V)^2}{(n - 1)(n + 2)p} \approx \frac{\sum \limits^k_{i=1} n_i \hat{a}_{2i}}{\sum \limits^k_{i=1} n_i} \xrightarrow{P} \frac{tr(\Sigma^2)}{p}$$


## Chaipitak and Chongcharoen 2013

- Chaipitak and Chongcharoen proposed a test in 2013 based on a distance estimator $b = tr(\Sigma^2_i) / tr(\Sigma^2_j)$.

$$T_{C} = \sum \limits^{k}_{i < j} \frac{(\hat{b} - 1)^2}{\hat{\delta}^2}$$
<br>

$$\hat{b} = \frac{\hat{a}_{2i}}{\hat{a}_{2j}} \quad \quad \hat{\delta}^2 = 4 \left( \frac{2\hat{a}_4}{p \hat{a}_2^2} \sum \limits^2_{i=1} \frac{1}{n_i -1} + \sum \limits^2_{i=1} \frac{1}{(n_i - 1)^2} \right)$$

## Srivastava et al. 2014

- Srivastava et al. proposed a test in 2014 that had the same form as Schott 2007 but used an unbiased consistent estimator for $\hat{a}_{2i}$.

<br>

$$ T_{S14} = \sum \limits^{k}_{i < j} \frac{(\hat{a}_{2i} + \hat{a}_{2j} - \frac{2}{p}tr(S_i S_j)) ^ 2}{\theta}$$
<br>
$$\hat{a}_{2i} = \frac{(n_i -2)(n_i -1)tr(V_i^2) - n(n - k)tr(D^2_i) + tr(V_i)^2}{pn_i(n_i -1)(n_i -2)(n_i -3)}$$

## Ishii et al. 2016

- Ishii et al proposed their test in 2016 using ratios of the first eigenvalues and first eigenvectors.

<br>

$$T_I = \prod \limits^{k}_{i < j} \tilde{\lambda}_* \tilde{h}_* \tilde{\gamma}_*$$

<br>

- Components of the test use noise reduced values since the largest eigenvalue is biased upwards with high dimensional data.

## Ishii et al. 2016

- The first noise reduced eigenvalue is solved by using the sum of the other non zero eigenvalues of the covariance matrix.

<br>

$$\tilde{\lambda}_* = \frac{max(\tilde{\lambda}_i, \tilde{\lambda}_j)}{min(\tilde{\lambda}_i, \tilde{\lambda}_j)}$$

<br>

$$\tilde{\lambda}_i = \hat{\lambda}_i - \frac{tr(S_D) - \sum \limits^i_{j = 1}\hat{\lambda}_j}{n - 1-i}$$

## Ishii et al. 2016

- The noise reduced first eigenvector is solved using the first eigenvalue.

<br>

$$\tilde{h}_* = max(|\tilde{h}^T_i \tilde{h}_j|, |\tilde{h}^T_i \tilde{h}_j|^{-1})$$

<br>

$$\tilde{h}_i = ((n-1)\tilde{\lambda}_i)^{-1/2}(\boldsymbol{X} - \boldsymbol{\overline{X}})\hat{\boldsymbol{\mu}}_i$$

## Ishii et al. 2016

- The final part of the test consists of the sum of all the other non zero eigenvalues of the covariance matrix.

<br>

$$\tilde{\gamma}_* = max(\frac{\tilde{\kappa}_1}{\tilde{\kappa}_2}, \frac{\tilde{\kappa}_2}{\tilde{\kappa}_1})$$

<br>

$$\tilde{\kappa}_i = tr(S_{D(i)}) - \tilde{\lambda}_{1(i)}$$
  
# Dimension Reduction via the Singular Value Decomposition

## Data Scatter Matrix 

- For samples $\boldsymbol{X}_i$, let 
  $\boldsymbol{X} = \left[ \boldsymbol{X}_1 \vdots \ \ldots \ \vdots \boldsymbol{X}_i\vdots \ \ldots \ \vdots \boldsymbol{X}_k \right]^T$.

- Let $\boldsymbol{V}_t$ represent the scatter matrix of $\boldsymbol{X}$, with rank $t$.

- Let $\boldsymbol{V}_t = \boldsymbol{UDU}^T$ represent the singular value decompostion of $\boldsymbol{V}_t$.

- Partition $\boldsymbol{U}$ such that $\boldsymbol{U} = \left[ \boldsymbol{U}_1 \vdots \, \boldsymbol{U}_2 
  \right]$ with $\boldsymbol{U}_1 \in \mathbb{R}^{p \times t}$.

- Project $\boldsymbol{X}_1$ and $\boldsymbol{X}_2$ to $t$ dimensions by taking $\boldsymbol{X}_{Ri}^T = \boldsymbol{U}_1^T 
  \boldsymbol{X}_i^T$.
  
## Covariance Difference Concatination Matrix 

- For groups $\boldsymbol{X}_i$ let $\boldsymbol{S_i}$ be the covariance matrix.

- Let $\widehat{\boldsymbol{M}} := \left[S_2 - S_1 \vdots \ldots \vdots S_i - S_1 \vdots \ldots \vdots S_k - S_1 \right]$ where $i = 1, \ldots, k$ groups.

- Let $\widehat{\boldsymbol{M}} = \boldsymbol{UDU}^T$ represent the singular value decompostion of $\widehat{\boldsymbol{M}}$.

- Partition $\boldsymbol{U}$ such that $\boldsymbol{U} = \left[ \boldsymbol{U}_1 \vdots \, \boldsymbol{U}_2 
  \right]$ with $\boldsymbol{U}_1 \in \mathbb{R}^{p \times t}$.

- Project $\boldsymbol{X}_1$ and $\boldsymbol{X}_2$ to $t$ dimensions by taking $\boldsymbol{X}_{Ri}^T = \boldsymbol{U}_1^T 
  \boldsymbol{X}_i^T$.

# Simulation Study Setup

## Covariance Structures

\[
  \boldsymbol{\Sigma} = \begin{bmatrix}
  1 & .5 & .25 & .125 \\
  .5 & 1 & .5 & .25 \\
  .25 & .5 & 1 & .5 \\
  .125 & .25 & .5 & 1 
  \end{bmatrix}.
  \]
  
  \[
  \boldsymbol{\Sigma} = \begin{bmatrix}
  20      & 1      & 3      & 3      & \\
  1       & \ddots & \ddots & 3      & \\
  3       & \ddots & 20     & 1      & \\
  3       & 3      & 1      & 5      & 4 \\
  3       & \ldots & 3      & 4      & \ddots \\
  \vdots  & \ddots & \vdots & \vdots & \ddots \\
  3       & \ldots & 3      & 4      & \ldots & 4
  \end{bmatrix}.
  \]

## Critical Value Simulation

The critical value data sets generated for the simulation study have the following characteristics:

- $n_i = 15$

- $p = 100$

- Generating k groups from $\mathcal{N}_p \left( \boldsymbol{0}, \boldsymbol{\Sigma} \right)$.

- Number of repetitions is 100000.

- $cv = inf\{x  \in  \mathbb{R}  :  1 - \alpha  \leq  \hat{F}_{T}(x) \}$, where $\alpha = .05$.

## Power Simulation

The power simulation data sets generated for the simulation study have the following characteristics:

- $n_i = 15$

- $p = 100$

- Generating k groups from $\mathcal{N}_p \left( \boldsymbol{0}, \boldsymbol{\Sigma} \right)$, and $\mathcal{N}_p \left( \boldsymbol{0}, \boldsymbol{\Sigma} * c \right)$.

- Number of repetitions is 1000.

$$power = \frac{\sum \limits^{1000}_{i = 1} I(T_i \geq cv)}{1000}$$

# Results

## Toeplitz 2

```{r, message = FALSE}
powertoe <- rename(filter(power, type == "toeplitz"), `Reduction Method` = Reduction.Method, Power = power)
ggplot(data = filter(powertoe,
                     populations == 2,
                     originaldimensions == 100,
                     ReducedDimension == 100,
                     Samples == 15,
                     Test %in% c("Modified Likelihood Ratio", "Chaipitak and Chongcharoen 2013", 
"Ishii et al. 2016", "Schott 2007", "Srivastava et al. 2014"),
                     `Reduction Method` %in% c("None"),
                     ReducedDimension %in% c(5, 100))) +
  geom_line(aes(x = `Frobenius Norm`, y = Power, color = Test, linetype = Test)) +
  theme_bw() 
```

## Toeplitz 2

```{r, message = FALSE}
ggplot(data = filter(powertoe,
                     populations == 2,
                     originaldimensions == 100,
                     Samples == 15,
                     Test %in% c("Modified Likelihood Ratio", "Chaipitak and Chongcharoen 2013", 
"Ishii et al. 2016", "Schott 2007", "Srivastava et al. 2014"),
                     `Reduction Method` %in% c("Covariance Differences", "Data Scatter"),
                     ReducedDimension %in% c(5, 100))) +
  geom_line(aes(x = `Frobenius Norm`, y = Power, color = Test, linetype = Test)) +
  facet_grid(`Reduction Method` ~ ReducedDimension) +
  theme_bw() 
```

## Toeplitz 2

```{r, message = FALSE}
ggplot(data = filter(powertoe,
                     populations == 2,
                     originaldimensions == 100,
                     Samples == 15,
                     Test %in% c("Modified Likelihood Ratio", "Chaipitak and Chongcharoen 2013", 
"Ishii et al. 2016", "Schott 2007", "Srivastava et al. 2014"),
                     `Reduction Method` %in% c("Covariance Differences", "Data Scatter"),
                     `Frobenius Norm` >= 0.01,
                     `Frobenius Norm` <= 0.014,
                     ReducedDimension <= 25)) +
  geom_line(aes(x = ReducedDimension, y = Power, color = Test, linetype = Test)) +
  facet_wrap(~ `Reduction Method`) +
  theme_bw() 
```

## Toeplitz 3

```{r, message = FALSE}
powertoe <- rename(filter(power, type == "toeplitz"), `Reduction Method` = Reduction.Method, Power = power)
ggplot(data = filter(powertoe,
                     populations == 3,
                     originaldimensions == 100,
                     ReducedDimension == 100,
                     Samples == 15,
                     Test %in% c("Modified Likelihood Ratio", "Chaipitak and Chongcharoen 2013", 
"Ishii et al. 2016", "Schott 2007", "Srivastava et al. 2014"),
                     `Reduction Method` %in% c("None"),
                     ReducedDimension %in% c(5, 100))) +
  geom_line(aes(x = `Frobenius Norm`, y = Power, color = Test, linetype = Test)) +
  theme_bw() 
```

## Toeplitz 3

```{r, message = FALSE}
ggplot(data = filter(powertoe,
                     populations == 3,
                     originaldimensions == 100,
                     Samples == 15,
                     Test %in% c("Modified Likelihood Ratio", "Chaipitak and Chongcharoen 2013", 
"Ishii et al. 2016", "Schott 2007", "Srivastava et al. 2014"),
                     `Reduction Method` %in% c("Covariance Differences", "Data Scatter"),
                     ReducedDimension %in% c(5, 100))) +
  geom_line(aes(x = `Frobenius Norm`, y = Power, color = Test, linetype = Test)) +
  facet_grid(`Reduction Method` ~ ReducedDimension) +
  theme_bw() 
```

## Toeplitz 3

```{r, message = FALSE}
ggplot(data = filter(powertoe,
                     populations == 3,
                     originaldimensions == 100,
                     Samples == 15,
                     Test %in% c("Modified Likelihood Ratio", "Chaipitak and Chongcharoen 2013", 
"Ishii et al. 2016", "Schott 2007", "Srivastava et al. 2014"),
                     `Reduction Method` %in% c("Covariance Differences", "Data Scatter"),
                     `Frobenius Norm` >= 0.015,
                     `Frobenius Norm` <= 0.02,
                     ReducedDimension <= 25)) +
  geom_line(aes(x = ReducedDimension, y = Power, color = Test, linetype = Test)) +
  facet_wrap(~ `Reduction Method`) +
  theme_bw() 
```

## Elliptical 2

```{r, message = FALSE}
powerell <- rename(filter(power, type == "elliptical"), `Reduction Method` = Reduction.Method, Power = power)
ggplot(data = filter(powerell,
                     populations == 2,
                     originaldimensions == 100,
                     ReducedDimension == 100,
                     Samples == 15,
                     Test %in% c("Modified Likelihood Ratio", "Chaipitak and Chongcharoen 2013", 
"Ishii et al. 2016", "Schott 2007", "Srivastava et al. 2014"),
                     `Reduction Method` %in% c("None"),
                     ReducedDimension %in% c(5, 100))) +
  geom_line(aes(x = `Frobenius Norm`, y = Power, color = Test, linetype = Test)) +
  theme_bw() 
```

## Elliptical 2

```{r, message = FALSE}
ggplot(data = filter(powerell,
                     populations == 2,
                     originaldimensions == 100,
                     Samples == 15,
                     Test %in% c("Modified Likelihood Ratio", "Chaipitak and Chongcharoen 2013", 
"Ishii et al. 2016", "Schott 2007", "Srivastava et al. 2014"),
                     `Reduction Method` %in% c("Covariance Differences", "Data Scatter"),
                     ReducedDimension %in% c(5, 100))) +
  geom_line(aes(x = `Frobenius Norm`, y = Power, color = Test, linetype = Test)) +
  facet_grid(`Reduction Method` ~ ReducedDimension) +
  theme_bw() 
```

## Elliptical 2

```{r, message = FALSE}
ggplot(data = filter(powerell,
                     populations == 2,
                     originaldimensions == 100,
                     Samples == 15,
                     Test %in% c("Modified Likelihood Ratio", "Chaipitak and Chongcharoen 2013", 
"Ishii et al. 2016", "Schott 2007", "Srivastava et al. 2014"),
                     `Reduction Method` %in% c("Covariance Differences", "Data Scatter"),
                     `Frobenius Norm` >= 0.75,
                     `Frobenius Norm` <= 0.85,
                     ReducedDimension <= 25)) +
  geom_line(aes(x = ReducedDimension, y = Power, color = Test, linetype = Test)) +
  facet_wrap(~ `Reduction Method`) +
  theme_bw() 
```

## Elliptical 3

```{r, message = FALSE}
ggplot(data = filter(powerell,
                     populations == 3,
                     originaldimensions == 100,
                     ReducedDimension == 100,
                     Samples == 15,
                     Test %in% c("Modified Likelihood Ratio", "Chaipitak and Chongcharoen 2013", 
"Ishii et al. 2016", "Schott 2007", "Srivastava et al. 2014"),
                     `Reduction Method` %in% c("None"),
                     ReducedDimension %in% c(5, 100))) +
  geom_line(aes(x = `Frobenius Norm`, y = Power, color = Test, linetype = Test)) +
  theme_bw() 
```

## Elliptical 3

```{r, message = FALSE}
ggplot(data = filter(powerell,
                     populations == 3,
                     originaldimensions == 100,
                     Samples == 15,
                     Test %in% c("Modified Likelihood Ratio", "Chaipitak and Chongcharoen 2013", 
"Ishii et al. 2016", "Schott 2007", "Srivastava et al. 2014"),
                     `Reduction Method` %in% c("Covariance Differences", "Data Scatter"),
                     ReducedDimension %in% c(5, 100))) +
  geom_line(aes(x = `Frobenius Norm`, y = Power, color = Test, linetype = Test)) +
  facet_grid(`Reduction Method` ~ ReducedDimension) +
  theme_bw() 
```

## Elliptical 3

```{r, message = FALSE}
ggplot(data = filter(powerell,
                     populations == 3,
                     originaldimensions == 100,
                     Samples == 15,
                     Test %in% c("Modified Likelihood Ratio", "Chaipitak and Chongcharoen 2013", 
"Ishii et al. 2016", "Schott 2007", "Srivastava et al. 2014"),
                     `Reduction Method` %in% c("Covariance Differences", "Data Scatter"),
                     `Frobenius Norm` >= 0.95,
                     `Frobenius Norm` <= 1.05,
                     ReducedDimension <= 25)) +
  geom_line(aes(x = ReducedDimension, y = Power, color = Test, linetype = Test)) +
  facet_wrap(~ `Reduction Method`) +
  theme_bw() 
```

# Summary and References

## Summary

- Dimension reduction improves power after some $n \gg m$???

- It appears the more pointed a covarince matrix becomes the worse the high dimension tests do and dimension reduction method doesn't help much

- Dimension reduction with the modified likelihood ratio test did well and with the pointed covariance structure even appeared to do better than the high dimensional tests.

## Future Work

- Explore some of the asymptotic properties a little better in some of these tests.

- Why does the Data Scatter dimension reduction method appear to do better than the covariance differences method.

- Looking at a another nontrivial k $(k \neq 3)$ group situation.

- Look at the Wald test with dimension reduction.

- Shrinkage estimators

- One sample tests

## References {.smaller}

  @schott_test_2007, @srivastava_tests_2014, @ishii_asymptotic_2016, 
  @chaipitak_test_2013, @ledoit_well-conditioned_2004
